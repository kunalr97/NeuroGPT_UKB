{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['encoder.patch_embedding.shallownet.0.weight', 'encoder.patch_embedding.shallownet.0.bias', 'encoder.patch_embedding.shallownet.1.weight', 'encoder.patch_embedding.shallownet.1.bias', 'encoder.patch_embedding.shallownet.2.weight', 'encoder.patch_embedding.shallownet.2.bias', 'encoder.patch_embedding.shallownet.2.running_mean', 'encoder.patch_embedding.shallownet.2.running_var', 'encoder.patch_embedding.shallownet.2.num_batches_tracked', 'encoder.patch_embedding.projection.0.weight', 'encoder.patch_embedding.projection.0.bias', 'encoder.transformer.0.0.fn.0.weight', 'encoder.transformer.0.0.fn.0.bias', 'encoder.transformer.0.0.fn.1.keys.weight', 'encoder.transformer.0.0.fn.1.keys.bias', 'encoder.transformer.0.0.fn.1.queries.weight', 'encoder.transformer.0.0.fn.1.queries.bias', 'encoder.transformer.0.0.fn.1.values.weight', 'encoder.transformer.0.0.fn.1.values.bias', 'encoder.transformer.0.0.fn.1.projection.weight', 'encoder.transformer.0.0.fn.1.projection.bias', 'encoder.transformer.0.1.fn.0.weight', 'encoder.transformer.0.1.fn.0.bias', 'encoder.transformer.0.1.fn.1.0.weight', 'encoder.transformer.0.1.fn.1.0.bias', 'encoder.transformer.0.1.fn.1.3.weight', 'encoder.transformer.0.1.fn.1.3.bias', 'encoder.transformer.1.0.fn.0.weight', 'encoder.transformer.1.0.fn.0.bias', 'encoder.transformer.1.0.fn.1.keys.weight', 'encoder.transformer.1.0.fn.1.keys.bias', 'encoder.transformer.1.0.fn.1.queries.weight', 'encoder.transformer.1.0.fn.1.queries.bias', 'encoder.transformer.1.0.fn.1.values.weight', 'encoder.transformer.1.0.fn.1.values.bias', 'encoder.transformer.1.0.fn.1.projection.weight', 'encoder.transformer.1.0.fn.1.projection.bias', 'encoder.transformer.1.1.fn.0.weight', 'encoder.transformer.1.1.fn.0.bias', 'encoder.transformer.1.1.fn.1.0.weight', 'encoder.transformer.1.1.fn.1.0.bias', 'encoder.transformer.1.1.fn.1.3.weight', 'encoder.transformer.1.1.fn.1.3.bias', 'encoder.transformer.2.0.fn.0.weight', 'encoder.transformer.2.0.fn.0.bias', 'encoder.transformer.2.0.fn.1.keys.weight', 'encoder.transformer.2.0.fn.1.keys.bias', 'encoder.transformer.2.0.fn.1.queries.weight', 'encoder.transformer.2.0.fn.1.queries.bias', 'encoder.transformer.2.0.fn.1.values.weight', 'encoder.transformer.2.0.fn.1.values.bias', 'encoder.transformer.2.0.fn.1.projection.weight', 'encoder.transformer.2.0.fn.1.projection.bias', 'encoder.transformer.2.1.fn.0.weight', 'encoder.transformer.2.1.fn.0.bias', 'encoder.transformer.2.1.fn.1.0.weight', 'encoder.transformer.2.1.fn.1.0.bias', 'encoder.transformer.2.1.fn.1.3.weight', 'encoder.transformer.2.1.fn.1.3.bias', 'encoder.transformer.3.0.fn.0.weight', 'encoder.transformer.3.0.fn.0.bias', 'encoder.transformer.3.0.fn.1.keys.weight', 'encoder.transformer.3.0.fn.1.keys.bias', 'encoder.transformer.3.0.fn.1.queries.weight', 'encoder.transformer.3.0.fn.1.queries.bias', 'encoder.transformer.3.0.fn.1.values.weight', 'encoder.transformer.3.0.fn.1.values.bias', 'encoder.transformer.3.0.fn.1.projection.weight', 'encoder.transformer.3.0.fn.1.projection.bias', 'encoder.transformer.3.1.fn.0.weight', 'encoder.transformer.3.1.fn.0.bias', 'encoder.transformer.3.1.fn.1.0.weight', 'encoder.transformer.3.1.fn.1.0.bias', 'encoder.transformer.3.1.fn.1.3.weight', 'encoder.transformer.3.1.fn.1.3.bias', 'encoder.transformer.4.0.fn.0.weight', 'encoder.transformer.4.0.fn.0.bias', 'encoder.transformer.4.0.fn.1.keys.weight', 'encoder.transformer.4.0.fn.1.keys.bias', 'encoder.transformer.4.0.fn.1.queries.weight', 'encoder.transformer.4.0.fn.1.queries.bias', 'encoder.transformer.4.0.fn.1.values.weight', 'encoder.transformer.4.0.fn.1.values.bias', 'encoder.transformer.4.0.fn.1.projection.weight', 'encoder.transformer.4.0.fn.1.projection.bias', 'encoder.transformer.4.1.fn.0.weight', 'encoder.transformer.4.1.fn.0.bias', 'encoder.transformer.4.1.fn.1.0.weight', 'encoder.transformer.4.1.fn.1.0.bias', 'encoder.transformer.4.1.fn.1.3.weight', 'encoder.transformer.4.1.fn.1.3.bias', 'encoder.transformer.5.0.fn.0.weight', 'encoder.transformer.5.0.fn.0.bias', 'encoder.transformer.5.0.fn.1.keys.weight', 'encoder.transformer.5.0.fn.1.keys.bias', 'encoder.transformer.5.0.fn.1.queries.weight', 'encoder.transformer.5.0.fn.1.queries.bias', 'encoder.transformer.5.0.fn.1.values.weight', 'encoder.transformer.5.0.fn.1.values.bias', 'encoder.transformer.5.0.fn.1.projection.weight', 'encoder.transformer.5.0.fn.1.projection.bias', 'encoder.transformer.5.1.fn.0.weight', 'encoder.transformer.5.1.fn.0.bias', 'encoder.transformer.5.1.fn.1.0.weight', 'encoder.transformer.5.1.fn.1.0.bias', 'encoder.transformer.5.1.fn.1.3.weight', 'encoder.transformer.5.1.fn.1.3.bias', 'embedder.msk_embed', 'embedder.cls_embed', 'embedder.embed_model.model.0.weight', 'embedder.embed_model.model.0.bias', 'embedder.embed_model.model.1.weight', 'embedder.embed_model.model.1.bias', 'decoder.transformer.wte.weight', 'decoder.transformer.wpe.weight', 'decoder.transformer.h.0.ln_1.weight', 'decoder.transformer.h.0.ln_1.bias', 'decoder.transformer.h.0.attn.c_attn.weight', 'decoder.transformer.h.0.attn.c_attn.bias', 'decoder.transformer.h.0.attn.c_proj.weight', 'decoder.transformer.h.0.attn.c_proj.bias', 'decoder.transformer.h.0.ln_2.weight', 'decoder.transformer.h.0.ln_2.bias', 'decoder.transformer.h.0.mlp.c_fc.weight', 'decoder.transformer.h.0.mlp.c_fc.bias', 'decoder.transformer.h.0.mlp.c_proj.weight', 'decoder.transformer.h.0.mlp.c_proj.bias', 'decoder.transformer.h.1.ln_1.weight', 'decoder.transformer.h.1.ln_1.bias', 'decoder.transformer.h.1.attn.c_attn.weight', 'decoder.transformer.h.1.attn.c_attn.bias', 'decoder.transformer.h.1.attn.c_proj.weight', 'decoder.transformer.h.1.attn.c_proj.bias', 'decoder.transformer.h.1.ln_2.weight', 'decoder.transformer.h.1.ln_2.bias', 'decoder.transformer.h.1.mlp.c_fc.weight', 'decoder.transformer.h.1.mlp.c_fc.bias', 'decoder.transformer.h.1.mlp.c_proj.weight', 'decoder.transformer.h.1.mlp.c_proj.bias', 'decoder.transformer.h.2.ln_1.weight', 'decoder.transformer.h.2.ln_1.bias', 'decoder.transformer.h.2.attn.c_attn.weight', 'decoder.transformer.h.2.attn.c_attn.bias', 'decoder.transformer.h.2.attn.c_proj.weight', 'decoder.transformer.h.2.attn.c_proj.bias', 'decoder.transformer.h.2.ln_2.weight', 'decoder.transformer.h.2.ln_2.bias', 'decoder.transformer.h.2.mlp.c_fc.weight', 'decoder.transformer.h.2.mlp.c_fc.bias', 'decoder.transformer.h.2.mlp.c_proj.weight', 'decoder.transformer.h.2.mlp.c_proj.bias', 'decoder.transformer.h.3.ln_1.weight', 'decoder.transformer.h.3.ln_1.bias', 'decoder.transformer.h.3.attn.c_attn.weight', 'decoder.transformer.h.3.attn.c_attn.bias', 'decoder.transformer.h.3.attn.c_proj.weight', 'decoder.transformer.h.3.attn.c_proj.bias', 'decoder.transformer.h.3.ln_2.weight', 'decoder.transformer.h.3.ln_2.bias', 'decoder.transformer.h.3.mlp.c_fc.weight', 'decoder.transformer.h.3.mlp.c_fc.bias', 'decoder.transformer.h.3.mlp.c_proj.weight', 'decoder.transformer.h.3.mlp.c_proj.bias', 'decoder.transformer.h.4.ln_1.weight', 'decoder.transformer.h.4.ln_1.bias', 'decoder.transformer.h.4.attn.c_attn.weight', 'decoder.transformer.h.4.attn.c_attn.bias', 'decoder.transformer.h.4.attn.c_proj.weight', 'decoder.transformer.h.4.attn.c_proj.bias', 'decoder.transformer.h.4.ln_2.weight', 'decoder.transformer.h.4.ln_2.bias', 'decoder.transformer.h.4.mlp.c_fc.weight', 'decoder.transformer.h.4.mlp.c_fc.bias', 'decoder.transformer.h.4.mlp.c_proj.weight', 'decoder.transformer.h.4.mlp.c_proj.bias', 'decoder.transformer.h.5.ln_1.weight', 'decoder.transformer.h.5.ln_1.bias', 'decoder.transformer.h.5.attn.c_attn.weight', 'decoder.transformer.h.5.attn.c_attn.bias', 'decoder.transformer.h.5.attn.c_proj.weight', 'decoder.transformer.h.5.attn.c_proj.bias', 'decoder.transformer.h.5.ln_2.weight', 'decoder.transformer.h.5.ln_2.bias', 'decoder.transformer.h.5.mlp.c_fc.weight', 'decoder.transformer.h.5.mlp.c_fc.bias', 'decoder.transformer.h.5.mlp.c_proj.weight', 'decoder.transformer.h.5.mlp.c_proj.bias', 'decoder.transformer.ln_f.weight', 'decoder.transformer.ln_f.bias', 'decoder.pooler_layer.0.weight', 'decoder.pooler_layer.0.bias', 'unembedder.model.0.weight', 'unembedder.model.0.bias'])\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'correct_key_here'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(model\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Extract embeddings from this model (replace 'transformer.tokens_embed.weight' with the correct key)\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcorrect_key_here\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m  \u001b[38;5;66;03m# Replace 'correct_key_here' with the actual key\u001b[39;00m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'correct_key_here'"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "model = torch.load('Neuro-GPT/pretrained_model/pytorch_model.bin', map_location=torch.device('cpu'))\n",
    "\n",
    "# List all keys in the state dictionary to find the correct key\n",
    "print(model.keys())\n",
    "\n",
    "# Extract embeddings from this model (replace 'transformer.tokens_embed.weight' with the correct key)\n",
    "embeddings = model['correct_key_here']  # Replace 'correct_key_here' with the actual key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 17744906\n",
      "ExampleModel(\n",
      "  (encoder): Encoder(\n",
      "    (patch_embedding): Sequential(\n",
      "      (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU(inplace=True)\n",
      "      (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (8): ReLU(inplace=True)\n",
      "    )\n",
      "    (transformer): ModuleList(\n",
      "      (0-5): 6 x TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (transformer): ModuleList(\n",
      "      (0-5): 6 x TransformerDecoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (multihead_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        (dropout3): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (fc): Linear(in_features=256, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Define the model architecture (example architecture, replace with actual)\n",
    "import torch.nn as nn\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper(Encoder, self).__init__()\n",
    "\t\tself.patch_embedding = nn.Sequential(\n",
    "\t\t\tnn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3),\n",
    "\t\t\tnn.BatchNorm2d(64),\n",
    "\t\t\tnn.ReLU(inplace=True),\n",
    "\t\t\tnn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\n",
    "\t\t\tnn.BatchNorm2d(128),\n",
    "\t\t\tnn.ReLU(inplace=True),\n",
    "\t\t\tnn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1),\n",
    "\t\t\tnn.BatchNorm2d(256),\n",
    "\t\t\tnn.ReLU(inplace=True)\n",
    "\t\t)\n",
    "\t\tself.transformer = nn.ModuleList([\n",
    "\t\t\tnn.TransformerEncoderLayer(d_model=256, nhead=8) for _ in range(6)\n",
    "\t\t])\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tx = self.patch_embedding(x)\n",
    "\t\tx = x.flatten(2).transpose(1, 2)\n",
    "\t\tfor layer in self.transformer:\n",
    "\t\t\tx = layer(x)\n",
    "\t\treturn x\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper(Decoder, self).__init__()\n",
    "\t\tself.transformer = nn.ModuleList([\n",
    "\t\t\tnn.TransformerDecoderLayer(d_model=256, nhead=8) for _ in range(6)\n",
    "\t\t])\n",
    "\t\tself.fc = nn.Linear(256, 10)\n",
    "\n",
    "\tdef forward(self, x, memory):\n",
    "\t\tfor layer in self.transformer:\n",
    "\t\t\tx = layer(x, memory)\n",
    "\t\tx = self.fc(x)\n",
    "\t\treturn x\n",
    "\n",
    "class ExampleModel(nn.Module):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper(ExampleModel, self).__init__()\n",
    "\t\tself.encoder = Encoder()\n",
    "\t\tself.decoder = Decoder()\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tmemory = self.encoder(x)\n",
    "\t\tx = self.decoder(x, memory)\n",
    "\t\treturn x\n",
    "\n",
    "# Initialize the model\n",
    "model_architecture = ExampleModel()\n",
    "\n",
    "# Load the state dictionary into the model\n",
    "model_architecture.load_state_dict(model, strict=False)\n",
    "\n",
    "# Find the number of parameters in the model\n",
    "total_params = sum(p.numel() for p in model_architecture.parameters())\n",
    "print(f'Total parameters: {total_params}')\n",
    "\n",
    "# Find the architecture of the model\n",
    "print(model_architecture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Layer Analysis:\n",
      "Input Channels: 1\n",
      "Output Channels: 40\n",
      "Kernel Size: 1x25\n"
     ]
    }
   ],
   "source": [
    "def analyze_input_layer(state_dict):\n",
    "    # Get the first convolutional layer weights\n",
    "    first_layer_weight = state_dict['encoder.patch_embedding.shallownet.0.weight']\n",
    "    \n",
    "    # Extract key dimensions\n",
    "    out_channels, in_channels, kernel_height, kernel_width = first_layer_weight.shape\n",
    "    \n",
    "    print(f\"First Layer Analysis:\")\n",
    "    print(f\"Input Channels: {in_channels}\")\n",
    "    print(f\"Output Channels: {out_channels}\")\n",
    "    print(f\"Kernel Size: {kernel_height}x{kernel_width}\")\n",
    "    \n",
    "    return {\n",
    "        'in_channels': in_channels,\n",
    "        'out_channels': out_channels,\n",
    "        'kernel_size': (kernel_height, kernel_width)\n",
    "    }\n",
    "\n",
    "# Analyze the model\n",
    "dims = analyze_input_layer(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Layer Analysis:\n",
      "Input Channels: 1\n",
      "Output Channels: 40\n",
      "Kernel Size: 1x25\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'in_channels': 1, 'out_channels': 40, 'kernel_size': (1, 25)}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyze_input_layer(model)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "UKB",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
