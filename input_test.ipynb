{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d20ccf01-12f0-44bb-8a52-cc61caa5ba96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "210e2ec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/kunal/UKB/Neuro_GPT/NeuroGPT_UKB/src\n"
     ]
    }
   ],
   "source": [
    "cd src/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce831806-963b-4747-b43b-86c14ddac7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from embedder import make \n",
    "from decoder import make_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5145d5d-02cd-4197-af62-c2afbdca8b5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunal/miniconda3/envs/UKB/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "e = make.make_embedder()\n",
    "d = make_decoder.make_decoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d12f907-f1fe-43d4-90fd-ddb88f1bcf20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import Model\n",
    "test = Model('Neuro-GPT/pretrained_model/pytorch_model.bin',e,d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96a5b4de-048a-4939-8ed8-69b1b08b8a2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Detailed Model Inspection:\n",
      "=========================\n",
      "\n",
      "Top-level attributes:\n",
      "T_destination: <class 'typing.TypeVar'>\n",
      "add_module: <class 'method'>\n",
      "apply: <class 'method'>\n",
      "bfloat16: <class 'method'>\n",
      "buffers: <class 'method'>\n",
      "call_super_init: <class 'bool'>\n",
      "children: <class 'method'>\n",
      "compile: <class 'method'>\n",
      "compute_loss: <class 'method'>\n",
      "cpu: <class 'method'>\n",
      "cuda: <class 'method'>\n",
      "decoder: <class 'decoder.gpt.GPTModel'>\n",
      "double: <class 'method'>\n",
      "dump_patches: <class 'bool'>\n",
      "embedder: <class 'embedder.csm.CSMEmbedder'>\n",
      "encoder: <class 'str'>\n",
      "eval: <class 'method'>\n",
      "extra_repr: <class 'method'>\n",
      "float: <class 'method'>\n",
      "forward: <class 'method'>\n",
      "from_pretrained: <class 'method'>\n",
      "ft_only_encoder: <class 'bool'>\n",
      "get_buffer: <class 'method'>\n",
      "get_extra_state: <class 'method'>\n",
      "get_parameter: <class 'method'>\n",
      "get_submodule: <class 'method'>\n",
      "half: <class 'method'>\n",
      "ipu: <class 'method'>\n",
      "is_decoding_mode: <class 'bool'>\n",
      "load_state_dict: <class 'method'>\n",
      "modules: <class 'method'>\n",
      "name: <class 'str'>\n",
      "named_buffers: <class 'method'>\n",
      "named_children: <class 'method'>\n",
      "named_modules: <class 'method'>\n",
      "named_parameters: <class 'method'>\n",
      "parameters: <class 'method'>\n",
      "prep_batch: <class 'method'>\n",
      "register_backward_hook: <class 'method'>\n",
      "register_buffer: <class 'method'>\n",
      "register_forward_hook: <class 'method'>\n",
      "register_forward_pre_hook: <class 'method'>\n",
      "register_full_backward_hook: <class 'method'>\n",
      "register_full_backward_pre_hook: <class 'method'>\n",
      "register_load_state_dict_post_hook: <class 'method'>\n",
      "register_module: <class 'method'>\n",
      "register_parameter: <class 'method'>\n",
      "register_state_dict_pre_hook: <class 'method'>\n",
      "requires_grad_: <class 'method'>\n",
      "set_extra_state: <class 'method'>\n",
      "share_memory: <class 'method'>\n",
      "state_dict: <class 'method'>\n",
      "switch_decoding_mode: <class 'method'>\n",
      "switch_ft_mode: <class 'method'>\n",
      "to: <class 'method'>\n",
      "to_empty: <class 'method'>\n",
      "train: <class 'method'>\n",
      "training: <class 'bool'>\n",
      "type: <class 'method'>\n",
      "unembedder: <class 'NoneType'>\n",
      "xpu: <class 'method'>\n",
      "zero_grad: <class 'method'>\n",
      "\n",
      "Embedder attributes:\n",
      "embedder.T_destination: <class 'typing.TypeVar'>\n",
      "embedder.add_cls_embed: <class 'method'>\n",
      "embedder.add_module: <class 'method'>\n",
      "embedder.apply: <class 'method'>\n",
      "embedder.bfloat16: <class 'method'>\n",
      "embedder.buffers: <class 'method'>\n",
      "embedder.bxe_loss: <class 'torch.nn.modules.loss.BCEWithLogitsLoss'>\n",
      "embedder.call_super_init: <class 'bool'>\n",
      "embedder.children: <class 'method'>\n",
      "embedder.cls_embed: <class 'torch.nn.parameter.Parameter'>\n",
      "embedder.compile: <class 'method'>\n",
      "embedder.cpu: <class 'method'>\n",
      "embedder.cuda: <class 'method'>\n",
      "embedder.decoding_loss: <class 'method'>\n",
      "embedder.double: <class 'method'>\n",
      "embedder.dropout: <class 'float'>\n",
      "embedder.dump_patches: <class 'bool'>\n",
      "embedder.embed_dim: <class 'int'>\n",
      "embedder.embed_inputs: <class 'method'>\n",
      "embedder.embed_model: <class 'embedder.base.EmbeddingModel'>\n",
      "embedder.eval: <class 'method'>\n",
      "embedder.extra_repr: <class 'method'>\n",
      "embedder.float: <class 'method'>\n",
      "embedder.forward: <class 'method'>\n",
      "embedder.get_buffer: <class 'method'>\n",
      "embedder.get_extra_state: <class 'method'>\n",
      "embedder.get_parameter: <class 'method'>\n",
      "embedder.get_submodule: <class 'method'>\n",
      "embedder.half: <class 'method'>\n",
      "embedder.in_dim: <class 'int'>\n",
      "embedder.in_dim_for_mask: <class 'int'>\n",
      "embedder.ipu: <class 'method'>\n",
      "embedder.is_decoding_mode: <class 'bool'>\n",
      "embedder.l1_loss: <class 'torch.nn.modules.loss.L1Loss'>\n",
      "embedder.l2_loss: <class 'torch.nn.modules.loss.MSELoss'>\n",
      "embedder.load_state_dict: <class 'method'>\n",
      "embedder.loss: <class 'method'>\n",
      "embedder.mask_inputs: <class 'method'>\n",
      "embedder.masking_loss: <class 'method'>\n",
      "embedder.modules: <class 'method'>\n",
      "embedder.msk_embed: <class 'torch.nn.parameter.Parameter'>\n",
      "embedder.name: <class 'str'>\n",
      "embedder.named_buffers: <class 'method'>\n",
      "embedder.named_children: <class 'method'>\n",
      "embedder.named_modules: <class 'method'>\n",
      "embedder.named_parameters: <class 'method'>\n",
      "embedder.num_hidden_layers: <class 'int'>\n",
      "embedder.parameters: <class 'method'>\n",
      "embedder.prep_batch: <class 'method'>\n",
      "embedder.reconstruction_loss: <class 'method'>\n",
      "embedder.register_backward_hook: <class 'method'>\n",
      "embedder.register_buffer: <class 'method'>\n",
      "embedder.register_forward_hook: <class 'method'>\n",
      "embedder.register_forward_pre_hook: <class 'method'>\n",
      "embedder.register_full_backward_hook: <class 'method'>\n",
      "embedder.register_full_backward_pre_hook: <class 'method'>\n",
      "embedder.register_load_state_dict_post_hook: <class 'method'>\n",
      "embedder.register_module: <class 'method'>\n",
      "embedder.register_parameter: <class 'method'>\n",
      "embedder.register_state_dict_pre_hook: <class 'method'>\n",
      "embedder.requires_grad_: <class 'method'>\n",
      "embedder.set_extra_state: <class 'method'>\n",
      "embedder.share_memory: <class 'method'>\n",
      "embedder.state_dict: <class 'method'>\n",
      "embedder.switch_decoding_mode: <class 'method'>\n",
      "embedder.to: <class 'method'>\n",
      "embedder.to_empty: <class 'method'>\n",
      "embedder.train: <class 'method'>\n",
      "embedder.training: <class 'bool'>\n",
      "embedder.training_style: <class 'str'>\n",
      "embedder.type: <class 'method'>\n",
      "embedder.xe_loss: <class 'torch.nn.modules.loss.CrossEntropyLoss'>\n",
      "embedder.xpu: <class 'method'>\n",
      "embedder.zero_grad: <class 'method'>\n",
      "\n",
      "Embed_model attributes:\n",
      "embedder.embed_model.T_destination: <class 'typing.TypeVar'>\n",
      "embedder.embed_model.add_module: <class 'method'>\n",
      "embedder.embed_model.apply: <class 'method'>\n",
      "embedder.embed_model.bfloat16: <class 'method'>\n",
      "embedder.embed_model.buffers: <class 'method'>\n",
      "embedder.embed_model.call_super_init: <class 'bool'>\n",
      "embedder.embed_model.children: <class 'method'>\n",
      "embedder.embed_model.compile: <class 'method'>\n",
      "embedder.embed_model.cpu: <class 'method'>\n",
      "embedder.embed_model.cuda: <class 'method'>\n",
      "embedder.embed_model.double: <class 'method'>\n",
      "embedder.embed_model.dropout: <class 'float'>\n",
      "embedder.embed_model.dump_patches: <class 'bool'>\n",
      "embedder.embed_model.embed_dim: <class 'int'>\n",
      "embedder.embed_model.eval: <class 'method'>\n",
      "embedder.embed_model.extra_repr: <class 'method'>\n",
      "embedder.embed_model.float: <class 'method'>\n",
      "embedder.embed_model.forward: <class 'method'>\n",
      "embedder.embed_model.get_buffer: <class 'method'>\n",
      "embedder.embed_model.get_extra_state: <class 'method'>\n",
      "embedder.embed_model.get_parameter: <class 'method'>\n",
      "embedder.embed_model.get_submodule: <class 'method'>\n",
      "embedder.embed_model.half: <class 'method'>\n",
      "embedder.embed_model.in_dim: <class 'int'>\n",
      "embedder.embed_model.ipu: <class 'method'>\n",
      "embedder.embed_model.load_state_dict: <class 'method'>\n",
      "embedder.embed_model.model: <class 'torch.nn.modules.container.Sequential'>\n",
      "embedder.embed_model.modules: <class 'method'>\n",
      "embedder.embed_model.named_buffers: <class 'method'>\n",
      "embedder.embed_model.named_children: <class 'method'>\n",
      "embedder.embed_model.named_modules: <class 'method'>\n",
      "embedder.embed_model.named_parameters: <class 'method'>\n",
      "embedder.embed_model.num_hidden_layers: <class 'int'>\n",
      "embedder.embed_model.parameters: <class 'method'>\n",
      "embedder.embed_model.register_backward_hook: <class 'method'>\n",
      "embedder.embed_model.register_buffer: <class 'method'>\n",
      "embedder.embed_model.register_forward_hook: <class 'method'>\n",
      "embedder.embed_model.register_forward_pre_hook: <class 'method'>\n",
      "embedder.embed_model.register_full_backward_hook: <class 'method'>\n",
      "embedder.embed_model.register_full_backward_pre_hook: <class 'method'>\n",
      "embedder.embed_model.register_load_state_dict_post_hook: <class 'method'>\n",
      "embedder.embed_model.register_module: <class 'method'>\n",
      "embedder.embed_model.register_parameter: <class 'method'>\n",
      "embedder.embed_model.register_state_dict_pre_hook: <class 'method'>\n",
      "embedder.embed_model.requires_grad_: <class 'method'>\n",
      "embedder.embed_model.set_extra_state: <class 'method'>\n",
      "embedder.embed_model.share_memory: <class 'method'>\n",
      "embedder.embed_model.state_dict: <class 'method'>\n",
      "embedder.embed_model.to: <class 'method'>\n",
      "embedder.embed_model.to_empty: <class 'method'>\n",
      "embedder.embed_model.train: <class 'method'>\n",
      "embedder.embed_model.training: <class 'bool'>\n",
      "embedder.embed_model.type: <class 'method'>\n",
      "embedder.embed_model.xpu: <class 'method'>\n",
      "embedder.embed_model.zero_grad: <class 'method'>\n",
      "\n",
      "Forward method:\n",
      "<bound method Model.forward of Model(\n",
      "  (embedder): CSMEmbedder(\n",
      "    (xe_loss): CrossEntropyLoss()\n",
      "    (bxe_loss): BCEWithLogitsLoss()\n",
      "    (l1_loss): L1Loss()\n",
      "    (l2_loss): MSELoss()\n",
      "    (embed_model): EmbeddingModel(\n",
      "      (model): Sequential(\n",
      "        (0): Linear(in_features=1024, out_features=768, bias=True)\n",
      "        (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): GPTModel(\n",
      "    (mse_loss): MSELoss()\n",
      "    (bxe_loss): BCEWithLogitsLoss()\n",
      "    (transformer): GPT2Model(\n",
      "      (wte): Embedding(1, 768)\n",
      "      (wpe): Embedding(512, 768)\n",
      "      (drop): Dropout(p=0.1, inplace=False)\n",
      "      (h): ModuleList(\n",
      "        (0-3): 4 x GPT2Block(\n",
      "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): GPT2Attention(\n",
      "            (c_attn): Conv1D()\n",
      "            (c_proj): Conv1D()\n",
      "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): GPT2MLP(\n",
      "            (c_fc): Conv1D()\n",
      "            (c_proj): Conv1D()\n",
      "            (act): NewGELUActivation()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (pooler_layer): Sequential(\n",
      "      (0): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (1): Tanh()\n",
      "      (2): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")>\n"
     ]
    }
   ],
   "source": [
    "def inspect_model_deeply(model):\n",
    "    \"\"\"\n",
    "    Deeply inspect the model's attributes and their types\n",
    "    \"\"\"\n",
    "    print(\"\\nDetailed Model Inspection:\")\n",
    "    print(\"=========================\")\n",
    "    \n",
    "    # Main model attributes\n",
    "    print(\"\\nTop-level attributes:\")\n",
    "    for attr in dir(model):\n",
    "        if not attr.startswith('_'):\n",
    "            value = getattr(model, attr)\n",
    "            print(f\"{attr}: {type(value)}\")\n",
    "            \n",
    "    # Inspect embedder if it exists\n",
    "    if hasattr(model, 'embedder'):\n",
    "        print(\"\\nEmbedder attributes:\")\n",
    "        embedder = model.embedder\n",
    "        for attr in dir(embedder):\n",
    "            if not attr.startswith('_'):\n",
    "                value = getattr(embedder, attr)\n",
    "                print(f\"embedder.{attr}: {type(value)}\")\n",
    "                \n",
    "        # Inspect embed_model if it exists\n",
    "        if hasattr(embedder, 'embed_model'):\n",
    "            print(\"\\nEmbed_model attributes:\")\n",
    "            embed_model = embedder.embed_model\n",
    "            for attr in dir(embed_model):\n",
    "                if not attr.startswith('_'):\n",
    "                    value = getattr(embed_model, attr)\n",
    "                    print(f\"embedder.embed_model.{attr}: {type(value)}\")\n",
    "\n",
    "    # Print the actual forward method if it exists\n",
    "    if hasattr(model, 'forward'):\n",
    "        print(\"\\nForward method:\")\n",
    "        print(model.forward)\n",
    "\n",
    "# Run the inspection\n",
    "inspect_model_deeply(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5104f84-bc1f-40d8-97e7-c4109ea38998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([500, 8, 1024])\n",
      "Attention mask shape: torch.Size([500, 8])\n",
      "Labels shape: torch.Size([500, 8, 1024])\n",
      "Output keys: dict_keys(['outputs'])\n",
      "outputs shape: torch.Size([500, 9, 768])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def create_dummy_input(batch_size=1, num_chunks=4, input_size=1024):\n",
    "    \"\"\"\n",
    "    Creates dummy input tensor with attention mask and labels\n",
    "    Args:\n",
    "        batch_size: Number of samples in the batch\n",
    "        num_chunks: Number of chunks per sample\n",
    "        input_size: Size matching the embedder's first Linear layer (1024)\n",
    "    \"\"\"\n",
    "    # Create random input tensor\n",
    "    dummy_input = torch.randn(batch_size, num_chunks, input_size)\n",
    "    \n",
    "    # Create attention mask (1 for valid tokens, 0 for padding)\n",
    "    attention_mask = torch.ones(batch_size, num_chunks, dtype=torch.long)\n",
    "    \n",
    "    # Create dummy labels tensor of the same shape as input\n",
    "    # We'll use zeros as dummy labels\n",
    "    labels = torch.zeros_like(dummy_input)\n",
    "    \n",
    "    # Create a batch dictionary as expected by the model\n",
    "    batch = {\n",
    "        'inputs': dummy_input,\n",
    "        'attention_mask': attention_mask,\n",
    "        'labels': labels\n",
    "    }\n",
    "    \n",
    "    return batch\n",
    "\n",
    "def get_model_outputs(model, batch):\n",
    "    \"\"\"\n",
    "    Get embeddings using the model's forward pass\n",
    "    \"\"\"\n",
    "    # Set model to eval mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Set encoder to None to avoid the string issue\n",
    "    model.encoder = None\n",
    "    \n",
    "    # Ensure we're not in training mode for masking\n",
    "    model.embedder.training_style = 'decoding'\n",
    "    model.is_decoding_mode = True\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Use the model's forward pass\n",
    "        outputs = model(batch)\n",
    "        return outputs\n",
    "\n",
    "def test_model(model, device='cpu'):\n",
    "    \"\"\"\n",
    "    Test the model with dummy input\n",
    "    \"\"\"\n",
    "    # Move model to specified device\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Create dummy input\n",
    "    batch = create_dummy_input(batch_size=500, num_chunks=8, input_size=1024)\n",
    "    \n",
    "    # Move all tensors to the same device\n",
    "    for key in batch:\n",
    "        if isinstance(batch[key], torch.Tensor):\n",
    "            batch[key] = batch[key].to(device)\n",
    "    \n",
    "    # Get embeddings\n",
    "    outputs = get_model_outputs(model, batch)\n",
    "    \n",
    "    print(f\"Input shape: {batch['inputs'].shape}\")\n",
    "    print(f\"Attention mask shape: {batch['attention_mask'].shape}\")\n",
    "    print(f\"Labels shape: {batch['labels'].shape}\")\n",
    "    print(f\"Output keys: {outputs.keys()}\")\n",
    "    \n",
    "    for key, value in outputs.items():\n",
    "        if isinstance(value, torch.Tensor):\n",
    "            print(f\"{key} shape: {value.shape}\")\n",
    "    \n",
    "    return outputs\n",
    "\n",
    "def apply_model(model, input, device='cpu'):\n",
    "    # Move model to specified device\n",
    "    model = model.to(device)\n",
    "    # Move all tensors to the same device\n",
    "    for key in input:\n",
    "        if isinstance(input[key], torch.Tensor):\n",
    "            input[key] = input[key].to(device)\n",
    "\n",
    "    \n",
    "\n",
    "# Run the test\n",
    "outputs = test_model(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7ac0757-71d9-4a60-a5cc-8dfa670aebc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from typing import Dict\n",
    "from scipy.io import loadmat\n",
    "import numpy as np\n",
    "\n",
    "class EEGDatasetFromMAT(Dataset):\n",
    "    def __init__(self, mat_file_path, chunk_len=500, num_chunks=8, ovlp=50, normalization=True):\n",
    "        # Load the .mat file\n",
    "        mat_data = loadmat(mat_file_path)\n",
    "        self.data = mat_data['data']  # Assuming data is stored in a variable named 'data'\n",
    "        self.labels = mat_data['data_labels'][0]  # Assuming labels are stored in 'data_labels'\n",
    "        \n",
    "        self.chunk_len = chunk_len\n",
    "        self.num_chunks = num_chunks\n",
    "        self.ovlp = ovlp\n",
    "        self.do_normalization = normalization\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[1]  # Number of samples or trials\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data_sample = self.data[idx]\n",
    "        if self.do_normalization:\n",
    "            data_sample = self.normalize(data_sample)\n",
    "        chunks = self.split_chunks(data_sample)\n",
    "        return chunks\n",
    "\n",
    "    def split_chunks(self, data, length=None, ovlp=None, num_chunks=None):\n",
    "        if length is None:\n",
    "            length = self.chunk_len\n",
    "        if ovlp is None:\n",
    "            ovlp = self.ovlp\n",
    "        if num_chunks is None:\n",
    "            num_chunks = self.num_chunks\n",
    "\n",
    "        all_chunks = []\n",
    "        total_len = data.shape[1]\n",
    "        actual_num_chunks = num_chunks\n",
    "        \n",
    "        if num_chunks * length > total_len - 1:\n",
    "            start_point = 0\n",
    "            actual_num_chunks = total_len // length\n",
    "        else:\n",
    "            start_point = np.random.randint(0, total_len - num_chunks * length)\n",
    "        \n",
    "        for _ in range(actual_num_chunks):\n",
    "            chunk = data[:, start_point: start_point + length]\n",
    "            all_chunks.append(np.array(chunk))\n",
    "            start_point += length - ovlp\n",
    "        \n",
    "        return np.array(all_chunks)\n",
    "\n",
    "    def normalize(self, data):\n",
    "        mean = np.mean(data, axis=-1, keepdims=True)\n",
    "        std = np.std(data, axis=-1, keepdims=True)\n",
    "        return (data - mean) / (std + 1e-25)\n",
    "    \n",
    "# Example usage\n",
    "mat_file_path = '/home/kunal/UKB/Neuro_GPT/NeuroGPT_UKB/data/001_data.mat'\n",
    "dataset = EEGDatasetFromMAT(mat_file_path)\n",
    "\n",
    "chunks = dataset.split_chunks(dataset.data)\n",
    "\n",
    "# Store chunks as a dictionary\n",
    "chunks_dict = {label[0]: chunks[:, i] for i, label in enumerate(dataset.labels)}\n",
    "# Add a new key with the following label: 'Oz', it is the average of O1 and O2\n",
    "chunks_dict['Oz'] = (chunks_dict['O1'] + chunks_dict['O2']) / 2\n",
    "# Store keys with the following labels:Fp1,Fp2,F7,F3,Fz,F4,F8,F9,T7,C3,Cz,C4,T8,F8,P7,P3,Pz,P4,P8,O1,Oz,O2\n",
    "chunk_filtered = {key: chunks_dict[key] for key in ['Fp1', 'Fp2', 'F7', 'F3', 'Fz', 'F4', 'F8', 'F9', 'T7', 'C3', 'Cz', 'C4', 'T8', 'F8', 'P7', 'P3', 'Pz', 'P4', 'P8', 'O1', 'Oz','O2']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "25a26fb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 500)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_filtered['Oz'].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "UKB",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
